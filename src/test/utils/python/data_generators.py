"""
Advanced test data generation module providing comprehensive capabilities for generating
realistic test data across all content types with validation and monitoring integration.

Version: 1.0.0
"""

# External imports with versions specified for security and compatibility
from uuid import UUID, uuid4  # latest
from datetime import datetime, timedelta  # latest
import random  # latest
from typing import Dict, List, Optional, Any
from faker import Faker  # v8.0.0
import logging

# Internal imports
from backend.content_discovery.app.models.content import Content, VALID_CONTENT_TYPES
from .test_helpers import load_test_data
from .test_logger import configure_test_logger

# Constants for data generation and validation
CONTENT_TYPES = VALID_CONTENT_TYPES
QUALITY_SCORE_RANGE = (0.0, 1.0)
DEFAULT_BATCH_SIZE = 10
DISTRIBUTION_PATTERNS = ['normal', 'uniform', 'weighted']
METADATA_SCHEMAS = {
    'video': {
        'duration': (60, 3600),  # 1 min to 1 hour in seconds
        'resolution': ['720p', '1080p', '1440p', '4K'],
        'platform': ['YouTube', 'Vimeo', 'Dailymotion'],
        'views': (100, 1000000)
    },
    'podcast': {
        'duration': (1200, 7200),  # 20 min to 2 hours in seconds
        'episode_number': (1, 500),
        'series_name': None,  # Will be generated by Faker
        'platform': ['Spotify', 'Apple Podcasts', 'Google Podcasts']
    },
    'article': {
        'word_count': (500, 5000),
        'publication_date': None,  # Will be generated dynamically
        'publisher': None,  # Will be generated by Faker
        'author': None  # Will be generated by Faker
    },
    'book': {
        'page_count': (100, 1000),
        'publication_year': (1990, datetime.now().year),
        'publisher': None,  # Will be generated by Faker
        'author': None,  # Will be generated by Faker
        'isbn': None  # Will be generated using ISBN-13 format
    }
}

class TestDataGenerator:
    """
    Advanced test data generator with comprehensive monitoring, validation,
    and fixture support for generating realistic test content.
    """

    def __init__(self, config: Optional[Dict] = None, logger: Optional[logging.Logger] = None):
        """
        Initializes the test data generator with configuration and monitoring.

        Args:
            config: Optional configuration overrides
            logger: Optional custom logger instance
        """
        self.faker = Faker()
        self.fixtures = {}
        self.logger = logger or configure_test_logger("test_data_generator")
        self.metrics = {
            "generated_items": 0,
            "validation_failures": 0,
            "performance_metrics": []
        }
        
        # Load fixtures if provided in config
        if config and 'fixtures_path' in config:
            self.fixtures = load_test_data(config['fixtures_path'])

    def create_content(self, fixture_name: str, overrides: Optional[Dict] = None) -> Content:
        """
        Creates a content item from a fixture with validation and monitoring.

        Args:
            fixture_name: Name of the fixture to use
            overrides: Optional override values

        Returns:
            Validated Content instance

        Raises:
            ValueError: If fixture is invalid or validation fails
        """
        start_time = datetime.now()
        
        try:
            # Load and validate fixture
            fixture_data = self.fixtures.get(fixture_name)
            if not fixture_data:
                raise ValueError(f"Fixture not found: {fixture_name}")

            # Apply overrides
            if overrides:
                fixture_data.update(overrides)

            # Generate content instance
            content = generate_content(
                content_type=fixture_data['type'],
                topic_id=fixture_data.get('topic_id'),
                quality_score=fixture_data.get('quality_score'),
                metadata_overrides=fixture_data.get('metadata')
            )

            # Track metrics
            self.metrics["generated_items"] += 1
            duration = (datetime.now() - start_time).total_seconds() * 1000
            self.metrics["performance_metrics"].append(duration)

            return content

        except Exception as e:
            self.metrics["validation_failures"] += 1
            self.logger.error(
                "Content generation failed",
                extra={
                    "fixture": fixture_name,
                    "error": str(e),
                    "duration_ms": (datetime.now() - start_time).total_seconds() * 1000
                }
            )
            raise

def generate_content(
    content_type: str,
    topic_id: Optional[UUID] = None,
    quality_score: Optional[float] = None,
    metadata_overrides: Optional[Dict] = None
) -> Content:
    """
    Generates a validated mock content item with realistic data.

    Args:
        content_type: Type of content to generate
        topic_id: Optional topic UUID
        quality_score: Optional quality score override
        metadata_overrides: Optional metadata overrides

    Returns:
        Validated Content instance

    Raises:
        ValueError: If content type is invalid or validation fails
    """
    if content_type not in CONTENT_TYPES:
        raise ValueError(f"Invalid content type. Must be one of: {CONTENT_TYPES}")

    # Generate or validate topic_id
    topic_id = topic_id or uuid4()

    # Generate base content data
    faker = Faker()
    title = faker.catch_phrase()
    description = faker.text(max_nb_chars=200)
    source_url = _generate_source_url(content_type, faker)
    
    # Generate or validate quality score
    if quality_score is None:
        quality_score = random.uniform(*QUALITY_SCORE_RANGE)
    elif not QUALITY_SCORE_RANGE[0] <= quality_score <= QUALITY_SCORE_RANGE[1]:
        raise ValueError(f"Quality score must be between {QUALITY_SCORE_RANGE[0]} and {QUALITY_SCORE_RANGE[1]}")

    # Generate metadata
    metadata = generate_metadata(content_type, metadata_overrides)

    # Create and validate content instance
    return Content(
        topic_id=topic_id,
        type=content_type,
        title=title,
        description=description,
        source_url=source_url,
        quality_score=quality_score,
        metadata=metadata
    )

def generate_content_batch(
    count: int,
    content_type: Optional[str] = None,
    topic_id: Optional[UUID] = None,
    batch_config: Optional[Dict] = None
) -> List[Content]:
    """
    Generates multiple validated content items with parallel processing support.

    Args:
        count: Number of items to generate
        content_type: Optional content type override
        topic_id: Optional topic UUID
        batch_config: Optional batch generation configuration

    Returns:
        List of validated Content instances
    """
    if count < 1:
        raise ValueError("Count must be positive")

    batch_config = batch_config or {}
    content_items = []
    
    # Select random content type if not specified
    if not content_type:
        content_type = random.choice(CONTENT_TYPES)

    for _ in range(count):
        try:
            item = generate_content(
                content_type=content_type,
                topic_id=topic_id,
                quality_score=batch_config.get('quality_score'),
                metadata_overrides=batch_config.get('metadata_overrides')
            )
            content_items.append(item)
        except Exception as e:
            logger = configure_test_logger("batch_generator")
            logger.error(f"Failed to generate content item: {str(e)}")
            continue

    return content_items

def generate_quality_scores(
    count: int,
    min_score: Optional[float] = None,
    max_score: Optional[float] = None,
    distribution: str = 'normal'
) -> List[float]:
    """
    Generates realistic quality scores with specified distribution patterns.

    Args:
        count: Number of scores to generate
        min_score: Optional minimum score
        max_score: Optional maximum score
        distribution: Distribution pattern to use

    Returns:
        List of normalized quality scores
    """
    min_score = min_score if min_score is not None else QUALITY_SCORE_RANGE[0]
    max_score = max_score if max_score is not None else QUALITY_SCORE_RANGE[1]

    if distribution not in DISTRIBUTION_PATTERNS:
        raise ValueError(f"Invalid distribution pattern. Must be one of: {DISTRIBUTION_PATTERNS}")

    scores = []
    if distribution == 'normal':
        mean = (min_score + max_score) / 2
        std_dev = (max_score - min_score) / 6
        scores = [
            min(max(random.gauss(mean, std_dev), min_score), max_score)
            for _ in range(count)
        ]
    elif distribution == 'uniform':
        scores = [
            random.uniform(min_score, max_score)
            for _ in range(count)
        ]
    elif distribution == 'weighted':
        weights = [0.1, 0.2, 0.4, 0.2, 0.1]
        ranges = [
            min_score + i * (max_score - min_score) / len(weights)
            for i in range(len(weights) + 1)
        ]
        scores = [
            random.uniform(ranges[i], ranges[i + 1])
            for i in random.choices(range(len(weights)), weights=weights, k=count)
        ]

    return scores

def generate_metadata(content_type: str, overrides: Optional[Dict] = None) -> Dict[str, Any]:
    """
    Generates comprehensive metadata for specific content types with validation.

    Args:
        content_type: Content type to generate metadata for
        overrides: Optional metadata overrides

    Returns:
        Validated content type specific metadata

    Raises:
        ValueError: If content type is invalid
    """
    if content_type not in METADATA_SCHEMAS:
        raise ValueError(f"Invalid content type: {content_type}")

    faker = Faker()
    schema = METADATA_SCHEMAS[content_type]
    metadata = {}

    for field, value_spec in schema.items():
        if overrides and field in overrides:
            metadata[field] = overrides[field]
            continue

        if isinstance(value_spec, tuple):
            if isinstance(value_spec[0], int):
                metadata[field] = random.randint(*value_spec)
            else:
                metadata[field] = random.uniform(*value_spec)
        elif isinstance(value_spec, list):
            metadata[field] = random.choice(value_spec)
        elif value_spec is None:
            if field == 'publication_date':
                metadata[field] = faker.date_between(
                    start_date='-1y',
                    end_date='today'
                ).isoformat()
            elif field in ('author', 'publisher'):
                metadata[field] = faker.name() if field == 'author' else faker.company()
            elif field == 'series_name':
                metadata[field] = faker.catch_phrase()
            elif field == 'isbn':
                metadata[field] = _generate_isbn13()

    return metadata

def _generate_source_url(content_type: str, faker: Faker) -> str:
    """Generates realistic source URLs based on content type."""
    if content_type == 'video':
        platform = random.choice(['youtube.com/watch?v=', 'vimeo.com/'])
        return f"https://www.{platform}{faker.uuid4()}"
    elif content_type == 'podcast':
        platform = random.choice(['spotify.com/episode/', 'podcasts.apple.com/episode/'])
        return f"https://{platform}{faker.uuid4()}"
    elif content_type == 'article':
        return f"https://{faker.domain_name()}/article/{faker.slug()}"
    else:  # book
        return f"https://books.google.com/books?id={faker.uuid4()}"

def _generate_isbn13() -> str:
    """Generates valid ISBN-13 number."""
    prefix = "978"
    digits = [int(d) for d in prefix] + [random.randint(0, 9) for _ in range(9)]
    
    # Calculate check digit
    check = sum((3 if i % 2 else 1) * d for i, d in enumerate(digits))
    check_digit = (10 - (check % 10)) % 10
    
    digits.append(check_digit)
    return "".join(map(str, digits))